{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intensive-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, AdamW, PretrainedConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "import atexit\n",
    "\n",
    "import logging\n",
    "from telegram import InlineKeyboardButton, InlineKeyboardMarkup\n",
    "import telegram\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nonprofit-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tokens(seqs):\n",
    "    \n",
    "    # очистка\n",
    "    for ind, seq in enumerate(seqs):\n",
    "        seq = re.sub(r\"http\\S+\", \"\", seq)\n",
    "        seq = re.sub(r\"http\", \"\", seq)\n",
    "        seq = re.sub(r\"@\\S+\", \"\", seq)\n",
    "        seq = re.sub(r\"#(\\w+)\", \"\", seq)\n",
    "        seq = re.sub(r\"[^A-Za-zА-Яа-яё0-9\\.\\!\\?\\...]\", \" \", seq)\n",
    "        seq = re.sub(r\"[a-zа-я0-9]+\\.[a-zа-я0-9]+\\.*[a-zа-я0-9]*\", \"\", seq)\n",
    "        seq = re.sub(r\"id\\w+\", \"\", seq)\n",
    "        seq = re.sub(r\"\\@\", \"at\", seq)\n",
    "        seqs[ind] = seq\n",
    "    \n",
    "    # лемматизация и тоенизация\n",
    "    X = []\n",
    "    for seq in tqdm(seqs):\n",
    "        new_seq = [\"[CLS]\"]\n",
    "        for word in word_tokenize(seq):\n",
    "            if word == '.':\n",
    "                new_seq.append('[SEP]')\n",
    "            elif '.' in word or len(re.findall(r'[0-9]+', word)) > 0:\n",
    "                pass\n",
    "            elif word not in russina_stop_words and word not in usa_stop_words:\n",
    "                new_seq.append(pymorphy2_analyzer.parse(word)[0].normal_form)\n",
    "\n",
    "        if new_seq[-1] != '[SEP]':\n",
    "            new_seq.append('[SEP]')\n",
    "        X.append(new_seq)\n",
    "\n",
    "    # паддинги\n",
    "\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in X],\n",
    "                maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=3)\n",
    "\n",
    "    # attention masks\n",
    "    attention_masks = []\n",
    "    for sent in input_ids:\n",
    "        att_mask = [int(token_id != 3) for token_id in sent]\n",
    "        # Store the attention mask for this sentence.\n",
    "        attention_masks.append(att_mask)\n",
    "        \n",
    "        \n",
    "    return torch.tensor(input_ids), torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "voluntary-heritage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/feodor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/feodor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('punkt')\n",
    "\n",
    "pymorphy2_analyzer = MorphAnalyzer()\n",
    "russina_stop_words = stopwords.words('russian')\n",
    "usa_stop_words = stopwords.words('english')\n",
    "tokenizer = BertTokenizer.from_pretrained('./vocab.txt')\n",
    "MAX_LEN = 128\n",
    "load_path = './bert_structEpoch1/'\n",
    "bert = BertForSequenceClassification.from_pretrained(str(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "specific-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    if not isinstance(text, list):\n",
    "        text = [text]\n",
    "    X, attention_masks = get_tokens(text)\n",
    "\n",
    "    # пробуем\n",
    "    bert.eval()\n",
    "    preds = bert(X, \n",
    "        token_type_ids=None, \n",
    "        attention_mask=attention_masks)\n",
    "    if not isinstance(text, list):\n",
    "        return(np.argmax(preds[0].detach().numpy(), axis=1).flatten()[0])\n",
    "    else:\n",
    "        return(np.argmax(preds[0].detach().numpy(), axis=1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modern-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def totg(texts):\n",
    "    pred = predict(texts)\n",
    "    return(list(pred))\n",
    "#     for i, predres in enumerate(list(pred)):\n",
    "#         prd = open('data/predictLog.csv','a')\n",
    "#         prd.write(\"{};{};{}\\n\".format(predres,post[i]['text'][:250].replace('\\n', '').replace('\\r', '').replace(';','').replace('\"',''),post[i]['wallUrl']))\n",
    "#         prd.close\n",
    "#         # json.dumps(post[i])\n",
    "# #         postTg(post[i], predres)\n",
    "#     print(predres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confirmed-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data/predictLog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indonesian-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "honest-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:00<00:00, 163.01it/s]\n"
     ]
    }
   ],
   "source": [
    "ans = totg(text[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intellectual-czech",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 150})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
