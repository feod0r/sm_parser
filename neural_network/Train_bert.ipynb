{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sacred-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, AdamW, PretrainedConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import random\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from torch import nn\n",
    "import string\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jewish-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\feodor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\feodor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>–•–æ—á—É —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ, –Ω–æ –Ω–µ –∑–Ω–∞—é, —á—Ç–æ –∏–º...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é!‚úåüèª  –ú–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–≥–µ–Ω–∏—è  –ü—Ä–∏–≥–ª–∞—à–∞—é ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>–í–ù–ò–ú–ê–ù–ò–ï!!! –°–ö–ò–î–ö–ê 5000 –†–£–ë–õ–ï–ô –ù–ê –ö–£–†–° –ü–ê–†–ò–ö–ú–ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>–° —Ü–µ–ª—å—é –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ –º–æ–ª–æ–¥–µ–∂–Ω–æ–π...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0   7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ ...\n",
       "1      0  –•–æ—á—É —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ, –Ω–æ –Ω–µ –∑–Ω–∞—é, —á—Ç–æ –∏–º...\n",
       "2      0  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é!‚úåüèª  –ú–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–≥–µ–Ω–∏—è  –ü—Ä–∏–≥–ª–∞—à–∞—é ...\n",
       "3      0  –í–ù–ò–ú–ê–ù–ò–ï!!! –°–ö–ò–î–ö–ê 5000 –†–£–ë–õ–ï–ô –ù–ê –ö–£–†–° –ü–ê–†–ò–ö–ú–ê...\n",
       "4      0  –° —Ü–µ–ª—å—é –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ –º–æ–ª–æ–¥–µ–∂–Ω–æ–π..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "data = pd.read_csv('./data/cleancorrectionw.csv',sep=';')\n",
    "#data = data.drop(columns=['Unnamed: 3', 'Unnamed: 4'])\n",
    "data = data.dropna()\n",
    "# data[\"text\"] = data[\"query\"] + '. ' + data[\"text\"]\n",
    "\n",
    "\n",
    "# queries = list(set(data[\"query\"].values))\n",
    "# data = data.drop(columns=['query'])\n",
    "data = data.rename({'class': 'label'}, axis='columns')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-latin",
   "metadata": {},
   "source": [
    "## Clear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "square-focus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>–•–æ—á—É —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ —Ç–æ –Ω–æ–≤–æ–µ –Ω–æ –Ω–µ –∑–Ω–∞—é —á—Ç–æ –∏–º–µ–Ω–Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–≥–µ–Ω–∏—è –ü—Ä–∏–≥–ª–∞—à–∞—é –í–∞—Å ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>–í–ù–ò–ú–ê–ù–ò–ï!!! –°–ö–ò–î–ö–ê 5000 –†–£–ë–õ–ï–ô –ù–ê –ö–£–†–° –ü–ê–†–ò–ö–ú–ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>–° —Ü–µ–ª—å—é –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ –º–æ–ª–æ–¥–µ–∂–Ω–æ–π...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0   7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ ...\n",
       "1      0  –•–æ—á—É —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ —Ç–æ –Ω–æ–≤–æ–µ –Ω–æ –Ω–µ –∑–Ω–∞—é —á—Ç–æ –∏–º–µ–Ω–Ω...\n",
       "2      0  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–≥–µ–Ω–∏—è –ü—Ä–∏–≥–ª–∞—à–∞—é –í–∞—Å ...\n",
       "3      0  –í–ù–ò–ú–ê–ù–ò–ï!!! –°–ö–ò–î–ö–ê 5000 –†–£–ë–õ–ï–ô –ù–ê –ö–£–†–° –ü–ê–†–ò–ö–ú–ê...\n",
       "4      0  –° —Ü–µ–ª—å—é –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ –º–æ–ª–æ–¥–µ–∂–Ω–æ–π..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"#(\\w+)\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z–ê-–Ø–∞-—è—ë0-9\\.\\!\\?\\...]\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"[a-z–∞-—è0-9]+\\.[a-z–∞-—è0-9]+\\.*[a-z–∞-—è0-9]*\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"id\\w+\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r'\\s+', ' ', regex=True)\n",
    "    return df\n",
    "\n",
    "clear_data = standardize_text(data.copy(), \"text\")\n",
    "clear_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "experimental-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear_data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "matched-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "behind-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chicken-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del lat text\n",
    "no_lat_inds = []\n",
    "for ind, seq in enumerate(clear_data.text.values):\n",
    "    is_lat = re.findall(r\"[A-Za-z]\\w+\", seq)\n",
    "    words = re.findall(r\"[–ê-–Ø–∞-—è]\\w+\", seq)\n",
    "    if len(is_lat) < len(words):\n",
    "        no_lat_inds.append(ind)\n",
    "\n",
    "X = clear_data.text.values[no_lat_inds]\n",
    "y = clear_data.label.values[no_lat_inds]\n",
    "# y[list(y).index(11)] = 1\n",
    "# y[list(y).index(4)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-tragedy",
   "metadata": {},
   "source": [
    "## –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rising-render",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 132/132 [00:01<00:00, 67.84it/s]\n"
     ]
    }
   ],
   "source": [
    "pymorphy2_analyzer = MorphAnalyzer()\n",
    "russina_stop_words = stopwords.words('russian')\n",
    "usa_stop_words = stopwords.words('english')\n",
    "vord_dict = []\n",
    "new_X = []\n",
    "for seq in tqdm(X):\n",
    "    new_seq = [\"[CLS]\"]\n",
    "    for word in word_tokenize(seq):\n",
    "        if word == '.':\n",
    "            new_seq.append('[SEP]')\n",
    "        elif '.' in word or len(re.findall(r'[0-9]+', word)) > 0:\n",
    "            pass\n",
    "        elif word not in russina_stop_words and word not in usa_stop_words:\n",
    "            new_seq.append(pymorphy2_analyzer.parse(word)[0].normal_form)\n",
    "            \n",
    "    if new_seq[-1] != '[SEP]':\n",
    "        new_seq.append('[SEP]')\n",
    "    new_X.append(new_seq)\n",
    "    vord_dict += new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fatal-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('vocab.txt', 'w') as f:\n",
    "#     for item in set(vord_dict):\n",
    "#         f.write(f'{item}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "boolean-coalition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feodor\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1614: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./vocab.txt')\n",
    "MAX_LEN = 128\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in new_X],\n",
    "            maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exciting-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attention_masks = []\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 3, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID not 3, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id != 3) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-corporation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "suffering-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 128) (105,) (27, 128) (27,)\n",
      "Counter({0: 97, 1: 8}) Counter({0: 26, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_ids, y,\n",
    "                                                    random_state=2021, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(Counter(y_train), Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "raising-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 128) (27, 128)\n"
     ]
    }
   ],
   "source": [
    "train_masks, test_masks, _, _ = train_test_split(np.array(attention_masks), y,\n",
    "                                             random_state=2018, test_size=0.2)\n",
    "print(train_masks.shape, test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eligible-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_train = torch.tensor(y_train).type(torch.LongTensor)\n",
    "y_test = torch.tensor(y_test).type(torch.LongTensor)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(X_train, train_masks, y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(X_test, test_masks, y_test)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eastern-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file('./bert_structv2/config.json')\n",
    "model = BertForSequenceClassification(config)\n",
    "#model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "breeding-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = './bert_structv2'\n",
    "bert = BertForSequenceClassification.from_pretrained(str(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-range",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bound-bradford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (54194, 768)\n",
      "bert.embeddings.position_embeddings.weight                (128, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(bert.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "authorized-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(bert.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 # adam_epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "european-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-helping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "every-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "exciting-singing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2617f7b2e10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "#torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "infectious-brunswick",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:49<00:00, 12.31s/it]\n",
      "Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.11s/it]\n",
      "C:\\Users\\feodor\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\feodor\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t train_loss : 0.29578690230846405 train_acc : 0.90625 val_acc : 0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:49<00:00, 12.43s/it]\n",
      "Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  2 \t train_loss : 0.2145996242761612 train_acc : 0.9453125 val_acc : 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "full_train_losses = []\n",
    "full_val_losses = []\n",
    "\n",
    "full_train_acc = []\n",
    "full_val_acc = []\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    t0 = time.time()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_rates = np.zeros(4)\n",
    "    val_rates = np.zeros(4)\n",
    "\n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    bert.train()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc='Train')):\n",
    "          \n",
    "        b_input_ids = batch[0]#.cuda()\n",
    "        b_input_mask = batch[1]#.cuda()\n",
    "        b_labels = batch[2]#.cuda()\n",
    "    \n",
    "        \n",
    "        bert.zero_grad()\n",
    "        \n",
    "        outputs = bert(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        train_correct += flat_accuracy(logits.detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(bert.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "#         avg_train_loss = total_loss / len(train_dataloader)\n",
    "#         loss_values.append(avg_train_loss)\n",
    "        \n",
    "        \n",
    "        t0 = time.time()\n",
    "        bert.eval()\n",
    "        \n",
    "        \n",
    "    for batch in tqdm(validation_dataloader, desc=\"Test\"):\n",
    "#         batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        val_correct += flat_accuracy(logits.detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n",
    "            \n",
    "    full_train_losses.append(np.mean(train_losses))\n",
    "    full_val_losses.append(np.mean(val_losses))\n",
    "\n",
    "    full_train_acc.append( (train_correct / len(train_dataloader)))\n",
    "    full_val_acc.append( (val_correct / len(validation_dataloader)))\n",
    "        \n",
    "    print('Epoch : ',epoch_i+1, '\\t', 'train_loss :', full_train_losses[-1].item(),\n",
    "              'train_acc :', full_train_acc[-1].item(), 'val_acc :', full_val_acc[-1].item())\n",
    "    \n",
    "    bert.save_pretrained('./bert_structEpoch'+str(epoch_i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cheap-richardson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  2 \t train_loss : 0.2145996242761612 train_acc : 0.9453125 val_acc : 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    " print('Epoch : ',epoch_i+1, '\\t', 'train_loss :', full_train_losses[-1].item(),\n",
    "              'train_acc :', full_train_acc[-1].item(), 'val_acc :', full_val_acc[-1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-truth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-fleet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
