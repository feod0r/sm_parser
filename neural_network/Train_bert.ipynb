{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaning-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, AdamW, PretrainedConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import random\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from torch import nn\n",
    "import string\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lucky-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\feodor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\feodor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>–•–æ—á—É —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ, –Ω–æ –Ω–µ –∑–Ω–∞—é, —á—Ç–æ –∏–º...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é!‚úåüèª  –ú–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–≥–µ–Ω–∏—è  –ü—Ä–∏–≥–ª–∞—à–∞—é ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>–í–ù–ò–ú–ê–ù–ò–ï!!! –°–ö–ò–î–ö–ê 5000 –†–£–ë–õ–ï–ô –ù–ê –ö–£–†–° –ü–ê–†–ò–ö–ú–ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>–° —Ü–µ–ª—å—é –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ –º–æ–ª–æ–¥–µ–∂–Ω–æ–π...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0   7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ ...\n",
       "1      0  –•–æ—á—É —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ-—Ç–æ –Ω–æ–≤–æ–µ, –Ω–æ –Ω–µ –∑–Ω–∞—é, —á—Ç–æ –∏–º...\n",
       "2      0  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é!‚úåüèª  –ú–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–≥–µ–Ω–∏—è  –ü—Ä–∏–≥–ª–∞—à–∞—é ...\n",
       "3      0  –í–ù–ò–ú–ê–ù–ò–ï!!! –°–ö–ò–î–ö–ê 5000 –†–£–ë–õ–ï–ô –ù–ê –ö–£–†–° –ü–ê–†–ò–ö–ú–ê...\n",
       "4      0  –° —Ü–µ–ª—å—é –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ –º–æ–ª–æ–¥–µ–∂–Ω–æ–π..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "data = pd.read_csv('./data/cleancorrectionw.csv',sep=';')\n",
    "#data = data.drop(columns=['Unnamed: 3', 'Unnamed: 4'])\n",
    "data = data.dropna()\n",
    "# data[\"text\"] = data[\"query\"] + '. ' + data[\"text\"]\n",
    "\n",
    "\n",
    "# queries = list(set(data[\"query\"].values))\n",
    "# data = data.drop(columns=['query'])\n",
    "data = data.rename({'class': 'label'}, axis='columns')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-albuquerque",
   "metadata": {},
   "source": [
    "## Clear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arranged-trading",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>–•–æ—á—É —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ —Ç–æ –Ω–æ–≤–æ–µ –Ω–æ –Ω–µ –∑–Ω–∞—é —á—Ç–æ –∏–º–µ–Ω–Ω...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–≥–µ–Ω–∏—è –ü—Ä–∏–≥–ª–∞—à–∞—é –í–∞—Å ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>–í–ù–ò–ú–ê–ù–ò–ï!!! –°–ö–ò–î–ö–ê 5000 –†–£–ë–õ–ï–ô –ù–ê –ö–£–†–° –ü–ê–†–ò–ö–ú–ê...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>–° —Ü–µ–ª—å—é –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ –º–æ–ª–æ–¥–µ–∂–Ω–æ–π...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0   7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ ...\n",
       "1      0  –•–æ—á—É —Å–¥–µ–ª–∞—Ç—å —á—Ç–æ —Ç–æ –Ω–æ–≤–æ–µ –Ω–æ –Ω–µ –∑–Ω–∞—é —á—Ç–æ –∏–º–µ–Ω–Ω...\n",
       "2      0  –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ï–≤–≥–µ–Ω–∏—è –ü—Ä–∏–≥–ª–∞—à–∞—é –í–∞—Å ...\n",
       "3      0  –í–ù–ò–ú–ê–ù–ò–ï!!! –°–ö–ò–î–ö–ê 5000 –†–£–ë–õ–ï–ô –ù–ê –ö–£–†–° –ü–ê–†–ò–ö–ú–ê...\n",
       "4      0  –° —Ü–µ–ª—å—é –ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏ –ø—Ä–µ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –≤ –º–æ–ª–æ–¥–µ–∂–Ω–æ–π..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"#(\\w+)\", \"\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z–ê-–Ø–∞-—è—ë0-9\\.\\!\\?\\...]\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"[a-z–∞-—è0-9]+\\.[a-z–∞-—è0-9]+\\.*[a-z–∞-—è0-9]*\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"id\\w+\", \" \", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\", regex=True)\n",
    "    df[text_field] = df[text_field].str.replace(r'\\s+', ' ', regex=True)\n",
    "    return df\n",
    "\n",
    "clear_data = standardize_text(data.copy(), \"text\")\n",
    "clear_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "infectious-headquarters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏ —Å—Ç–∞—Ä—Ç–∞ —Å–∏—Å—Ç–µ–º—ã –†–∞–∑—Ä–µ—à–∏—Ç–µ –º–Ω–µ –≤—Å–µ–≥–æ –ª–∏—à—å –¥–≤–µ –º–∏–Ω—É—Ç–∫–∏ –∏ —è –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é –∫–∞–∫ –í—ã —Å—É–º–µ–µ—Ç–µ —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç 7 000 —Ä—É–± —É–∂–µ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è. –ó–∞—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ—Ç—Ä–∞—Ç–∏–≤ –Ω–∞ –≤—Å–µ —ç—Ç–æ –Ω–µ —Å–≤—ã—à–µ 40 –º–∏–Ω –ª–∏—á–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏! —Å–µ–º—å –¢—ã—â –∑–∞ 24 —á–∞—Å–∞? –ù–µ—Ä–µ–∞–ª—å–Ω–æ ! –¢—ã —Ç–∞–∫ —Å—á–∏—Ç–∞–µ—à—å? –ù–µ —Å–ø–µ—à–∏ —Å –æ—Ç–≤–µ—Ç–∞–º–∏. –¢–æ —á—Ç–æ –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –Ω–µ–ª—å–∑—è –∑–∞–∫–æ–ª–æ—Ç–∏—Ç—å –¥–æ–≤–æ–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –º–∏—Ñ –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—é—Ç –ª–µ–Ω—Ç—è–∏ –ª–∏–±–æ –Ω–µ—É–¥–∞—á–Ω–∏–∫–∏. –ï—Å–ª–∏ —É–∂ —Ç—ã —Ç–æ—á–Ω–æ —Ö–æ—á–µ—à—å –∑–∞–Ω—è—Ç—å—Å—è —á–∞—Å—Ç–Ω—ã–º –±–∏–∑–Ω–µ—Å–æ–º –∫–æ–Ω—á–∞—Ç—å –≥–æ—Ä–±–∏—Ç—å —Å–ø–∏–Ω—É –Ω–∞ –¥—è–¥—é –æ—Å–≤–æ–±–æ–¥–∏—Ç—Å—è –æ—Ç –¥–æ–ª–∂–∫–æ–≤ –ø–æ–∑–∞–±—ã—Ç—å –æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å—Ä–µ–¥—Å—Ç–≤ –æ—Ç—ã—Å–∫–∞—Ç—å —Å–µ–±—è –∏–º–µ—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ —Å–≤–æ–∏ –º–µ—á—Ç—ã –¢—ã —Å–º–æ–∂–µ—à—å —ç—Ç–æ –æ—Å—É—â–µ—Å—Ç–≤–∏—Ç—å! –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–¥–æ–±–Ω—ã–º–∏ –∫—É—Ä—Å–∞–º–∏ 1 —ã–µ –∑–∞—Ä–∞–±–æ—Ç–∫–∏ —Å—Ä–∞–∑—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤. –ü–æ—Å—Ç–∏–≥–Ω—É—Ç—å –∏ –ø—É—Å—Ç–∏—Ç—å –≤ –¥–µ–ª–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª –º–æ–∂–µ—Ç –≤—Å—è–∫–∏–π –Ω–∞—á–∏–Ω–∞—è –æ—Ç –ø–æ–¥—Ä–æ—Å—Ç–∫–∞ –¥–æ –ø–µ–Ω—Å–∏–æ–Ω–µ—Ä–∞. –ì–∞—Ä–∞–Ω—Ç–∏–π–Ω–æ–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 200 –ü—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å –Ω–µ —Å—Ç–∞–Ω–µ—Ç –ø–∞–¥–∞—Ç—å –∫–æ–≥–¥–∞ –≤—ã –∑–∞—Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –¥—É—Ö. –í—ã —Å—Ç–∞–Ω–µ—Ç–µ –ø–æ–ª—É—á–∞—Ç—å –¥–µ–Ω—å–≥–∏ –∏ –í–∞—Å –∑–∞ —ç—Ç–æ –ø–æ–º–∏–º–æ —Ç–æ–≥–æ –Ω–∞—á–Ω—É—Ç –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å! –≠—Ç–æ –≤ —Ü–µ–ª–æ–º –∫—Ä—É—Ç–æ! –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤ —Ç–µ—á–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ –≥–æ–¥–∞. –í—ã—Ä—É—á–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç—Å—è! –î–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å—Ç–∞—Ä–∞—Ç—å—Å—è –¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ–¥–∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –∏ –∑–∞—Ç–µ–º –µ–≥–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏—à—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å. –ì–∞—Ä–∞–Ω—Ç–∏–π–Ω–æ–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤. –ú–µ–Ω—è –∑–æ–≤—É—Ç –î–º–∏—Ç—Ä–∏–π –ê–ª–µ–º–∞—Å–æ–≤ –º–Ω–µ 31 –≥–æ–¥ –∂–∏–≤—É —è –≤ –≥–æ—Ä–æ–¥–µ –°–∞–º–∞—Ä–∞. –ò —è —Ä–∞—Å—Å–∫–∞–∂—É –≤–∞–º –æ –º–µ—Ç–æ–¥–∏–∫–µ –¥–æ—Ö–æ–¥–æ–≤ –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —É–ø–æ—Ç—Ä–µ–±–ª—è—è –∫–æ—Ç–æ—Ä—É—é —è –∑–∞—Ä–∞–±–æ—Ç–∞–ª —Ä—É–± –∑–∞ 2 –º–µ—Å—è—Ü–∞. –Ø —Ç—Ä–∞—á—É –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ –±–æ–ª–µ–µ —á–µ–º 1 2 —á–∞—Å–∞ –≤ –¥–µ–Ω—å –¥–ª—è —Ç–æ–≥–æ —á—Ç–æ –±—ã –∫–∞–∂–¥—ã–π –±–ª–∏–∂–∞–π—à–∏–π –¥–µ–Ω—å –Ω–∞–¥–µ–∂–Ω–æ –≤—ã—Ä—É—á–∞—Ç—å –æ—Ç 7000 —Ä—É–±. –Ø –Ω–∏–∫–æ–º—É –Ω–µ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–ª –æ —ç—Ç–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ —Ä–∞–Ω—å—à–µ –∏ —è –∫—Ä–∞–π–Ω–µ —Å–∏–ª—å–Ω–æ —Å–æ–º–Ω–µ–≤–∞—é—Å—å —á—Ç–æ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —á—Ç–æ —Ç–æ –±–æ–ª–µ–µ –¥–æ—Ö–æ–¥–Ω–æ–µ —á–µ–º —Ç–æ —á—Ç–æ —Å–æ–≤–µ—Ç—É—é –¥–ª—è –≤–∞—Å —è. –Ø –ø–µ—Ä–µ–ø—Ä–æ–±–æ–≤–∞–ª –æ—á–µ–Ω—å –º–Ω–æ–≥–æ —Å–ø–æ—Å–æ–±–æ–≤ —Ä–µ–∫–ª–∞–º–∏—Ä—É–µ–º—ã—Ö –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –æ–¥–Ω–∞–∫–æ —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–∞–ª –º–Ω–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫–æ–≤—ã–º–∏ —è –¥–æ –∫–æ–Ω—Ü–∞ –¥–æ–≤–æ–ª–µ–Ω.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "leading-butterfly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 7000 —Ä—É–± –∑–∞ 24 —á–∞—Å–∞! –ü–µ—Ä–≤—ã–µ –¥–µ–Ω—å–≥–∏ —Ç–æ—Ç—á–∞—Å –∂–µ –ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏ —Å—Ç–∞—Ä—Ç–∞ —Å–∏—Å—Ç–µ–º—ã –†–∞–∑—Ä–µ—à–∏—Ç–µ –º–Ω–µ –≤—Å–µ–≥–æ-–ª–∏—à—å –¥–≤–µ –º–∏–Ω—É—Ç–∫–∏, –∏ —è –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É—é –∫–∞–∫ –í—ã —Å—É–º–µ–µ—Ç–µ —É—Å–ø–µ—à–Ω–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç 7 000 —Ä—É–± —É–∂–µ –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è.  –ó–∞—Ä–∞–±–æ—Ç–∞—Ç—å, –ø–æ—Ç—Ä–∞—Ç–∏–≤ –Ω–∞ –≤—Å–µ —ç—Ç–æ –Ω–µ —Å–≤—ã—à–µ 40 –º–∏–Ω –ª–∏—á–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏! ¬´—Å–µ–º—å –¢—ã—â –∑–∞ 24 —á–∞—Å–∞? –ù–µ—Ä–µ–∞–ª—å–Ω–æ¬ª! –¢—ã —Ç–∞–∫ —Å—á–∏—Ç–∞–µ—à—å? –ù–µ —Å–ø–µ—à–∏ —Å –æ—Ç–≤–µ—Ç–∞–º–∏. –¢–æ, —á—Ç–æ –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –Ω–µ–ª—å–∑—è –∑–∞–∫–æ–ª–æ—Ç–∏—Ç—å –¥–æ–≤–æ–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ ‚Äì –º–∏—Ñ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—é—Ç –ª–µ–Ω—Ç—è–∏ –ª–∏–±–æ –Ω–µ—É–¥–∞—á–Ω–∏–∫–∏. –ï—Å–ª–∏ —É–∂ —Ç—ã —Ç–æ—á–Ω–æ —Ö–æ—á–µ—à—å: - –∑–∞–Ω—è—Ç—å—Å—è —á–∞—Å—Ç–Ω—ã–º –±–∏–∑–Ω–µ—Å–æ–º - –∫–æ–Ω—á–∞—Ç—å –≥–æ—Ä–±–∏—Ç—å —Å–ø–∏–Ω—É –Ω–∞ ¬´–¥—è–¥—é¬ª - –æ—Å–≤–æ–±–æ–¥–∏—Ç—Å—è –æ—Ç –¥–æ–ª–∂–∫–æ–≤ - –ø–æ–∑–∞–±—ã—Ç—å –æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å—Ä–µ–¥—Å—Ç–≤ - –æ—Ç—ã—Å–∫–∞—Ç—å —Å–µ–±—è - –∏–º–µ—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥ - –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ —Å–≤–æ–∏ –º–µ—á—Ç—ã ‚Ä¶–¢—ã —Å–º–æ–∂–µ—à—å —ç—Ç–æ –æ—Å—É—â–µ—Å—Ç–≤–∏—Ç—å! –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø–æ–¥–æ–±–Ω—ã–º–∏ –∫—É—Ä—Å–∞–º–∏: - 1-—ã–µ –∑–∞—Ä–∞–±–æ—Ç–∫–∏ —Å—Ä–∞–∑—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –ø—Ä–∏–æ–±—Ä–µ—Ç–µ–Ω–Ω—ã—Ö –Ω–∞–≤—ã–∫–æ–≤. - –ü–æ—Å—Ç–∏–≥–Ω—É—Ç—å –∏ –ø—É—Å—Ç–∏—Ç—å –≤ –¥–µ–ª–æ —ç—Ç–æ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª –º–æ–∂–µ—Ç –≤—Å—è–∫–∏–π, –Ω–∞—á–∏–Ω–∞—è –æ—Ç –ø–æ–¥—Ä–æ—Å—Ç–∫–∞ –¥–æ –ø–µ–Ω—Å–∏–æ–Ω–µ—Ä–∞. - –ì–∞—Ä–∞–Ω—Ç–∏–π–Ω–æ–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ 200% - –ü—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å –Ω–µ —Å—Ç–∞–Ω–µ—Ç –ø–∞–¥–∞—Ç—å, –∫–æ–≥–¥–∞ –≤—ã –∑–∞—Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –¥—É—Ö. - –í—ã —Å—Ç–∞–Ω–µ—Ç–µ –ø–æ–ª—É—á–∞—Ç—å –¥–µ–Ω—å–≥–∏, –∏ –í–∞—Å –∑–∞ —ç—Ç–æ –ø–æ–º–∏–º–æ —Ç–æ–≥–æ –Ω–∞—á–Ω—É—Ç –±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å! –≠—Ç–æ –≤ —Ü–µ–ª–æ–º –∫—Ä—É—Ç–æ! - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤ —Ç–µ—á–µ–Ω–∏–∏ –æ–¥–Ω–æ–≥–æ –≥–æ–¥–∞. - –í—ã—Ä—É—á–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç—Å—è! –î–∞ , –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω—É–∂–Ω–æ –Ω–µ–º–Ω–æ–≥–æ –ø–æ—Å—Ç–∞—Ä–∞—Ç—å—Å—è,–¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ–¥–∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç –º–µ—Ö–∞–Ω–∏–∑–º, –∏ –∑–∞—Ç–µ–º –µ–≥–æ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –ª–∏—à—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å. - –ì–∞—Ä–∞–Ω—Ç–∏–π–Ω–æ–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è —Å—Ä–µ–¥—Å—Ç–≤. –ú–µ–Ω—è –∑–æ–≤—É—Ç –î–º–∏—Ç—Ä–∏–π –ê–ª–µ–º–∞—Å–æ–≤, –º–Ω–µ 31 –≥–æ–¥, –∂–∏–≤—É —è –≤ –≥–æ—Ä–æ–¥–µ –°–∞–º–∞—Ä–∞. –ò —è —Ä–∞—Å—Å–∫–∞–∂—É –≤–∞–º –æ –º–µ—Ç–æ–¥–∏–∫–µ –¥–æ—Ö–æ–¥–æ–≤ –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —É–ø–æ—Ç—Ä–µ–±–ª—è—è –∫–æ—Ç–æ—Ä—É—é —è –∑–∞—Ä–∞–±–æ—Ç–∞–ª 327.000 —Ä—É–± –∑–∞ 2 –º–µ—Å—è—Ü–∞. –Ø —Ç—Ä–∞—á—É –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ –±–æ–ª–µ–µ —á–µ–º 1-2 —á–∞—Å–∞ –≤ –¥–µ–Ω—å, –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ –±—ã –∫–∞–∂–¥—ã–π –±–ª–∏–∂–∞–π—à–∏–π –¥–µ–Ω—å –Ω–∞–¥–µ–∂–Ω–æ –≤—ã—Ä—É—á–∞—Ç—å –æ—Ç 7000 —Ä—É–±. –Ø –Ω–∏–∫–æ–º—É –Ω–µ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–ª –æ —ç—Ç–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ —Ä–∞–Ω—å—à–µ, –∏ —è –∫—Ä–∞–π–Ω–µ —Å–∏–ª—å–Ω–æ —Å–æ–º–Ω–µ–≤–∞—é—Å—å, —á—Ç–æ –≤—ã –Ω–∞–π–¥–µ—Ç–µ —á—Ç–æ-—Ç–æ –±–æ–ª–µ–µ –¥–æ—Ö–æ–¥–Ω–æ–µ, —á–µ–º —Ç–æ, —á—Ç–æ —Å–æ–≤–µ—Ç—É—é –¥–ª—è –≤–∞—Å —è. –Ø –ø–µ—Ä–µ–ø—Ä–æ–±–æ–≤–∞–ª –æ—á–µ–Ω—å –º–Ω–æ–≥–æ —Å–ø–æ—Å–æ–±–æ–≤, —Ä–µ–∫–ª–∞–º–∏—Ä—É–µ–º—ã—Ö –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –æ–¥–Ω–∞–∫–æ —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–∞–ª –º–Ω–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∫–∞–∫–æ–≤—ã–º–∏ —è –¥–æ –∫–æ–Ω—Ü–∞ –¥–æ–≤–æ–ª–µ–Ω.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wrapped-geology",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-30553a2da0f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "increasing-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del lat text\n",
    "no_lat_inds = []\n",
    "for ind, seq in enumerate(clear_data.text.values):\n",
    "    is_lat = re.findall(r\"[A-Za-z]\\w+\", seq)\n",
    "    words = re.findall(r\"[–ê-–Ø–∞-—è]\\w+\", seq)\n",
    "    if len(is_lat) < len(words):\n",
    "        no_lat_inds.append(ind)\n",
    "\n",
    "X = clear_data.text.values[no_lat_inds]\n",
    "y = clear_data.label.values[no_lat_inds]\n",
    "# y[list(y).index(11)] = 1\n",
    "# y[list(y).index(4)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-chapel",
   "metadata": {},
   "source": [
    "## –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "injured-graham",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 132/132 [00:01<00:00, 69.08it/s]\n"
     ]
    }
   ],
   "source": [
    "pymorphy2_analyzer = MorphAnalyzer()\n",
    "russina_stop_words = stopwords.words('russian')\n",
    "usa_stop_words = stopwords.words('english')\n",
    "vord_dict = []\n",
    "new_X = []\n",
    "for seq in tqdm(X):\n",
    "    new_seq = [\"[CLS]\"]\n",
    "    for word in word_tokenize(seq):\n",
    "        if word == '.':\n",
    "            new_seq.append('[SEP]')\n",
    "        elif '.' in word or len(re.findall(r'[0-9]+', word)) > 0:\n",
    "            pass\n",
    "        elif word not in russina_stop_words and word not in usa_stop_words:\n",
    "            new_seq.append(pymorphy2_analyzer.parse(word)[0].normal_form)\n",
    "            \n",
    "    if new_seq[-1] != '[SEP]':\n",
    "        new_seq.append('[SEP]')\n",
    "    new_X.append(new_seq)\n",
    "    vord_dict += new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "serious-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for item in set(vord_dict):\n",
    "        f.write(f'{item}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "superior-scoop",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feodor\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1614: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./vocab.txt')\n",
    "MAX_LEN = 128\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in new_X],\n",
    "            maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "better-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "attention_masks = []\n",
    "# For each sentence...\n",
    "for sent in input_ids:\n",
    "    \n",
    "    # Create the attention mask.\n",
    "    #   - If a token ID is 3, then it's padding, set the mask to 0.\n",
    "    #   - If a token ID not 3, then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id != 3) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-newton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "challenging-daily",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 128) (105,) (27, 128) (27,)\n",
      "Counter({0: 97, 1: 8}) Counter({0: 26, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input_ids, y,\n",
    "                                                    random_state=2021, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(Counter(y_train), Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "chicken-notebook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 128) (27, 128)\n"
     ]
    }
   ],
   "source": [
    "train_masks, test_masks, _, _ = train_test_split(np.array(attention_masks), y,\n",
    "                                             random_state=2018, test_size=0.2)\n",
    "print(train_masks.shape, test_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "understood-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_train = torch.tensor(y_train).type(torch.LongTensor)\n",
    "y_test = torch.tensor(y_test).type(torch.LongTensor)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "minor-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(X_train, train_masks, y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(X_test, test_masks, y_test)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "marked-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file('./bert_structv2/config.json')\n",
    "model = BertForSequenceClassification(config)\n",
    "#model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hydraulic-industry",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = './bert_structv2'\n",
    "bert = BertForSequenceClassification.from_pretrained(str(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-pixel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "optical-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (54194, 768)\n",
      "bert.embeddings.position_embeddings.weight                (128, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(bert.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "brilliant-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(bert.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 # adam_epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "constitutional-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-canvas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "animated-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "minus-mayor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1597d79b350>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "#torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "random-blogger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:48<00:00, 12.17s/it]\n",
      "Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t train_loss : 0.29578690230846405 train_acc : 0.90625 val_acc : 0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:48<00:00, 12.04s/it]\n",
      "Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:03<00:00,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  2 \t train_loss : 0.2145996242761612 train_acc : 0.9453125 val_acc : 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "full_train_losses = []\n",
    "full_val_losses = []\n",
    "\n",
    "full_train_acc = []\n",
    "full_val_acc = []\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    t0 = time.time()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_rates = np.zeros(4)\n",
    "    val_rates = np.zeros(4)\n",
    "\n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    bert.train()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc='Train')):\n",
    "          \n",
    "        b_input_ids = batch[0]#.cuda()\n",
    "        b_input_mask = batch[1]#.cuda()\n",
    "        b_labels = batch[2]#.cuda()\n",
    "    \n",
    "        \n",
    "        bert.zero_grad()\n",
    "        \n",
    "        outputs = bert(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        train_correct += flat_accuracy(logits.detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(bert.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "#         avg_train_loss = total_loss / len(train_dataloader)\n",
    "#         loss_values.append(avg_train_loss)\n",
    "        \n",
    "        \n",
    "        t0 = time.time()\n",
    "        bert.eval()\n",
    "        \n",
    "        \n",
    "    for batch in tqdm(validation_dataloader, desc=\"Test\"):\n",
    "#         batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        val_correct += flat_accuracy(logits.detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n",
    "            \n",
    "    full_train_losses.append(np.mean(train_losses))\n",
    "    full_val_losses.append(np.mean(val_losses))\n",
    "\n",
    "    full_train_acc.append( (train_correct / len(train_dataloader)))\n",
    "    full_val_acc.append( (val_correct / len(validation_dataloader)))\n",
    "        \n",
    "    print('Epoch : ',epoch_i+1, '\\t', 'train_loss :', full_train_losses[-1].item(),\n",
    "              'train_acc :', full_train_acc[-1].item(), 'val_acc :', full_val_acc[-1].item())\n",
    "    \n",
    "    bert.save_pretrained('./bert_structEpoch'+str(epoch_i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acting-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  2 \t train_loss : 0.2145996242761612 train_acc : 0.9453125 val_acc : 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    " print('Epoch : ',epoch_i+1, '\\t', 'train_loss :', full_train_losses[-1].item(),\n",
    "              'train_acc :', full_train_acc[-1].item(), 'val_acc :', full_val_acc[-1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-reliance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|‚ñâ         | 11/115 [05:17<48:57, 28.25s/it]"
     ]
    }
   ],
   "source": [
    "full_train_losses = []\n",
    "full_val_losses = []\n",
    "\n",
    "full_train_acc = []\n",
    "full_val_acc = []\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    t0 = time.time()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_rates = np.zeros(4)\n",
    "    val_rates = np.zeros(4)\n",
    "\n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc='Train')):\n",
    "          \n",
    "        b_input_ids = batch[0]#.cuda()\n",
    "        b_input_mask = batch[1]#.cuda()\n",
    "        b_labels = batch[2]#.cuda()\n",
    "    \n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        train_correct += flat_accuracy(logits.detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "#         avg_train_loss = total_loss / len(train_dataloader)\n",
    "#         loss_values.append(avg_train_loss)\n",
    "        \n",
    "        \n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "    for batch in tqdm(validation_dataloader, desc=\"Test\"):\n",
    "#         batch = tuple(t.cuda() for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        val_correct += flat_accuracy(logits.detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n",
    "            \n",
    "    full_train_losses.append(np.mean(train_losses))\n",
    "    full_val_losses.append(np.mean(val_losses))\n",
    "\n",
    "    full_train_acc.append( (train_correct / len(train_dataloader)))\n",
    "    full_val_acc.append( (val_correct / len(validation_dataloader)))\n",
    "        \n",
    "    print('Epoch : ',epoch_i+1, '\\t', 'train_loss :', full_train_losses[-1].item(),\n",
    "              'train_acc :', full_train_acc[-1].item(), 'val_acc :', full_val_acc[-1].item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "concerned-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.save_pretrained('./bert_structv3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "guilty-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = './bert_struct'\n",
    "bert = BertForSequenceClassification.from_pretrained(str(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "modular-racing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(54348, 768, padding_idx=3)\n",
       "      (position_embeddings): Embedding(128, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-french",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smoking-morrison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in '/Users/feodor/nltk_data/corpora/stopwords'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-entity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-harassment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-benjamin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-choice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-bosnia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-samuel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-accuracy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-definition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-following",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-programmer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-institution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-indian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-trade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-benchmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-defense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-willow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-photograph",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-criterion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-seattle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-gregory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-nancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-faith",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-science",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-replication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-indication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-bradford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-camping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-summary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-explosion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-signature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-purse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-novel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-hobby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-slide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-biography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-notification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-minimum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-corpus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-median",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-feedback",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
